{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/dnanhkhoa/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import gensim, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "from gensim import utils\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from pytorch_pretrained_bert import BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "\n",
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "        \n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self,x_y_list, transform=None):\n",
    "        \n",
    "        self.x_y_list = x_y_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
    "        \n",
    "        if len(tokenized_review) > max_seq_length:\n",
    "            tokenized_review = tokenized_review[:max_seq_length]\n",
    "            \n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "\n",
    "        padding = [0] * (max_seq_length - len(ids_review))\n",
    "        \n",
    "        ids_review += padding\n",
    "        \n",
    "        assert len(ids_review) == max_seq_length\n",
    "        \n",
    "        #print(ids_review)\n",
    "        ids_review = torch.tensor(ids_review)\n",
    "        \n",
    "        labels = self.x_y_list[1][index] # color        \n",
    "        list_of_labels = [torch.from_numpy(np.array(labels))]\n",
    "        \n",
    "        \n",
    "        return ids_review, list_of_labels[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size_or_config_json_file=29794, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "num_labels = 2\n",
    "max_seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    \"\"\"\n",
    "        Simple preprocessing. \n",
    "        We found out that taking to much time preprocessing tweets decreased the classification performance.\n",
    "        This needs further investigation, but an possible explanation would be that with preprocessing we lose\n",
    "        too much information.\n",
    "    \"\"\"\n",
    "    \n",
    "    s = str(s)\n",
    "    s = unidecode(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "    best_acc = 0\n",
    "    acc_train = 0\n",
    "    acc_train_min = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            corrects = 0\n",
    "            \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, vec in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device) \n",
    "\n",
    "                vec = vec.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    outputs = F.softmax(outputs,dim=1)\n",
    "                    \n",
    "                    loss = criterion(outputs, torch.max(vec.float(), 1)[1])\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                \n",
    "                corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(vec, 1)[1])\n",
    "\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            \n",
    "            acc = corrects.double() / dataset_sizes[phase]\n",
    "            if phase == 'val':\n",
    "                epoch_acc = acc\n",
    "            else:\n",
    "                acc_train = acc\n",
    "\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} acc: {:.4f}'.format(phase, acc))\n",
    "\n",
    "#             if phase == 'val' and epoch_loss < best_loss:\n",
    "            \n",
    "            if acc_train >= acc_train_min and phase == 'val' and (epoch_acc > best_acc or epoch_acc == best_acc and epoch_loss < best_loss):\n",
    "            \n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_final.pth')\n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_dataloaders(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "\n",
    "    y_train = pd.get_dummies(y_train).values.tolist()\n",
    "    y_test = pd.get_dummies(y_test).values.tolist()\n",
    "\n",
    "    train_lists = [X_train, y_train]\n",
    "    test_lists = [X_test, y_test]\n",
    "\n",
    "    training_dataset = TweetDataset(x_y_list = train_lists )\n",
    "\n",
    "    test_dataset = TweetDataset(x_y_list = test_lists )\n",
    "\n",
    "    dataloaders_dict = {\n",
    "        'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "        'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    }\n",
    "    dataset_sizes = {'train':len(train_lists[0]),\n",
    "                    'val':len(test_lists[0])}\n",
    "    \n",
    "    \n",
    "    return [dataloaders_dict, dataset_sizes]\n",
    "\n",
    "lrlast = .0001\n",
    "lrmain = .00001\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "dat = pd.read_csv('final_train.csv')\n",
    "dat = dat.dropna()\n",
    "\n",
    "X = dat['tweet'].to_numpy()#.apply(lambda x: clean_text(x))\n",
    "y = dat['label'].to_numpy()\n",
    "\n",
    "acc_values = []\n",
    "f1_scores = []\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "k = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"CROSS VALIDATION K = %d\" % (k))\n",
    "    k += 1\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    model = BertForSequenceClassification(num_labels)\n",
    "    model.to(device)\n",
    "    \n",
    "    optim1 = optim.Adam([\n",
    "            {\"params\":model.bert.parameters(), \"lr\": lrmain},\n",
    "            {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    optimizer_ft = optim1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataloaders_dict, dataset_sizes = get_dataloaders(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_kfold = train_model(model, criterion, optimizer_ft, num_epochs=epochs)\n",
    "    model_kfold.to(device)\n",
    "    model_kfold.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        Perform predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = X_test\n",
    "    labels = y_test\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True, max_length=64,\n",
    "                                             pad_to_max_length=True, return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler,\n",
    "                                       batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            \n",
    "        logits = outputs\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        \n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "    predictions = np.argmax(np.concatenate(predictions, axis=0), axis=1).flatten()\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    acc = accuracy_score(predictions, true_labels)\n",
    "    f1s = f1_score(predictions, true_labels)\n",
    "    \n",
    "    print(\"\\n>> TEST -  ACC: %.3f, F1-Score: %.3f\" % (acc, f1s))\n",
    "    \n",
    "    acc_values.append(acc)\n",
    "    f1_scores.append(f1s)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results = pd.DataFrame({\n",
    "    'acc': acc_values,\n",
    "    'f1_score': f1_scores\n",
    "})\n",
    "pd_results.to_csv('results/BERT.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
