{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/dnanhkhoa/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1021 09:10:55.916649 16096 file_utils.py:41] PyTorch version 1.9.0+cpu available.\n",
      "I1021 09:11:01.964714 16096 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import gensim, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "from gensim import utils\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from pytorch_pretrained_bert import BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "\n",
    "# from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1021 09:11:08.451135 16096 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/config.json from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\aac3429673975db22f5d8a9202bc6a8983145bbd621577c9f2f62bee7fd02934.c6449db73a9350063f76a64baf5b26ca3759c9435babbd865baa989b009eb662\n",
      "I1021 09:11:08.453136 16096 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "I1021 09:11:08.455140 16096 tokenization_utils.py:420] Model name 'neuralmind/bert-base-portuguese-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'neuralmind/bert-base-portuguese-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I1021 09:11:10.988557 16096 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/vocab.txt from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\953e203ca70d433ab232eb85ae0093b7fd73a61d7931d54a460a131b5e15b10e.d2f3b3fde3658e304f905ed624bcacd8555c946377121f9211932f66c9fe1202\n",
      "I1021 09:11:10.989613 16096 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/added_tokens.json from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\e763f2a607ad8e4a10a0a8ca9b7e289e8704d9bf24ee520772ee28547721c295.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I1021 09:11:10.990577 16096 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/special_tokens_map.json from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\aa29d58ec2f0278e3ff71ce26a902511a549e75a7596ae790a751253f2950b3d.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I1021 09:11:10.990577 16096 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/tokenizer_config.json from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\316240c7b41ed25c9fc43a68d281f3f84a6efdb7f40cf1e9bd739e859285991b.d3d276ada459b0aec3a25156bf4df869f8362968dd3a7052748ad41fcb496476\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "        \n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self,x_y_list, transform=None):\n",
    "        \n",
    "        self.x_y_list = x_y_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
    "        \n",
    "        if len(tokenized_review) > max_seq_length:\n",
    "            tokenized_review = tokenized_review[:max_seq_length]\n",
    "            \n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "\n",
    "        padding = [0] * (max_seq_length - len(ids_review))\n",
    "        \n",
    "        ids_review += padding\n",
    "        \n",
    "        assert len(ids_review) == max_seq_length\n",
    "        \n",
    "        #print(ids_review)\n",
    "        ids_review = torch.tensor(ids_review)\n",
    "        \n",
    "        labels = self.x_y_list[1][index] # color        \n",
    "        list_of_labels = [torch.from_numpy(np.array(labels))]\n",
    "        \n",
    "        \n",
    "        return ids_review, list_of_labels[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size_or_config_json_file=29794, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "num_labels = 2\n",
    "max_seq_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    \"\"\"\n",
    "        Simple preprocessing. \n",
    "        We found out that taking to much time preprocessing tweets decreased the classification performance.\n",
    "        This needs further investigation, but an possible explanation would be that with preprocessing we lose\n",
    "        too much information.\n",
    "    \"\"\"\n",
    "    \n",
    "    s = str(s)\n",
    "    s = unidecode(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "    best_acc = 0\n",
    "    acc_train = 0\n",
    "    acc_train_min = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            corrects = 0\n",
    "            \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, vec in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device) \n",
    "\n",
    "                vec = vec.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    outputs = F.softmax(outputs,dim=1)\n",
    "                    \n",
    "                    loss = criterion(outputs, torch.max(vec.float(), 1)[1])\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                \n",
    "                corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(vec, 1)[1])\n",
    "\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            \n",
    "            acc = corrects.double() / dataset_sizes[phase]\n",
    "            if phase == 'val':\n",
    "                epoch_acc = acc\n",
    "            else:\n",
    "                acc_train = acc\n",
    "\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} acc: {:.4f}'.format(phase, acc))\n",
    "\n",
    "#             if phase == 'val' and epoch_loss < best_loss:\n",
    "            \n",
    "            if acc_train >= acc_train_min and phase == 'val' and (epoch_acc > best_acc or epoch_acc == best_acc and epoch_loss < best_loss):\n",
    "            \n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_final.pth')\n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION K = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1021 09:41:06.483866 16096 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/config.json from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\aac3429673975db22f5d8a9202bc6a8983145bbd621577c9f2f62bee7fd02934.c6449db73a9350063f76a64baf5b26ca3759c9435babbd865baa989b009eb662\n",
      "I1021 09:41:06.486843 16096 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "I1021 09:41:07.114312 16096 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/pytorch_model.bin from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\6c81c033cb3ac2c92420708c858bd910dacfafa6bc5f482d5eb9ff07486cc0b2.ef5eff44e441799ed0962f2e2b7d6314c155c84a8602daf06b5ca4c219d5decc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/0\n",
      "----------\n",
      "train total loss: 0.5709 \n",
      "train acc: 0.7209\n",
      "val total loss: 0.5075 \n",
      "val acc: 0.8078\n",
      "saving with loss of 0.5075496615103956 improved over previous 100\n",
      "\n",
      "Training complete in 10m 22s\n",
      "Best val Acc: 0.507550\n",
      "\n",
      ">> TEST -  ACC: 0.713, F1-Score: 0.770\n",
      "\n",
      "\n",
      "\n",
      "CROSS VALIDATION K = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1021 09:53:27.584227 16096 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/config.json from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\aac3429673975db22f5d8a9202bc6a8983145bbd621577c9f2f62bee7fd02934.c6449db73a9350063f76a64baf5b26ca3759c9435babbd865baa989b009eb662\n",
      "I1021 09:53:27.584227 16096 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "I1021 09:53:28.222348 16096 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/pytorch_model.bin from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\6c81c033cb3ac2c92420708c858bd910dacfafa6bc5f482d5eb9ff07486cc0b2.ef5eff44e441799ed0962f2e2b7d6314c155c84a8602daf06b5ca4c219d5decc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/0\n",
      "----------\n",
      "train total loss: 0.5737 \n",
      "train acc: 0.7199\n",
      "val total loss: 0.5264 \n",
      "val acc: 0.7795\n",
      "saving with loss of 0.5263782143592834 improved over previous 100\n",
      "\n",
      "Training complete in 10m 6s\n",
      "Best val Acc: 0.526378\n",
      "\n",
      ">> TEST -  ACC: 0.785, F1-Score: 0.818\n",
      "\n",
      "\n",
      "\n",
      "CROSS VALIDATION K = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1021 10:05:32.709018 16096 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/config.json from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\aac3429673975db22f5d8a9202bc6a8983145bbd621577c9f2f62bee7fd02934.c6449db73a9350063f76a64baf5b26ca3759c9435babbd865baa989b009eb662\n",
      "I1021 10:05:32.712074 16096 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "I1021 10:05:33.348898 16096 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/neuralmind/bert-base-portuguese-cased/pytorch_model.bin from cache at C:\\Users\\Windows\\.cache\\torch\\transformers\\6c81c033cb3ac2c92420708c858bd910dacfafa6bc5f482d5eb9ff07486cc0b2.ef5eff44e441799ed0962f2e2b7d6314c155c84a8602daf06b5ca4c219d5decc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 0/0\n",
      "----------\n",
      "train total loss: 0.5754 \n",
      "train acc: 0.7199\n",
      "val total loss: 0.5165 \n",
      "val acc: 0.7854\n",
      "saving with loss of 0.5165004179162799 improved over previous 100\n",
      "\n",
      "Training complete in 10m 25s\n",
      "Best val Acc: 0.516500\n",
      "\n",
      ">> TEST -  ACC: 0.805, F1-Score: 0.828\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_dataloaders(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "\n",
    "    y_train = pd.get_dummies(y_train).values.tolist()\n",
    "    y_test = pd.get_dummies(y_test).values.tolist()\n",
    "\n",
    "    train_lists = [X_train, y_train]\n",
    "    test_lists = [X_test, y_test]\n",
    "\n",
    "    training_dataset = TweetDataset(x_y_list = train_lists )\n",
    "\n",
    "    test_dataset = TweetDataset(x_y_list = test_lists )\n",
    "\n",
    "    dataloaders_dict = {\n",
    "        'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "        'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    }\n",
    "    dataset_sizes = {'train':len(train_lists[0]),\n",
    "                    'val':len(test_lists[0])}\n",
    "    \n",
    "    \n",
    "    return [dataloaders_dict, dataset_sizes]\n",
    "\n",
    "lrlast = .0001\n",
    "lrmain = .00001\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "dat = pd.read_csv('final_train.csv')\n",
    "dat = dat.dropna()\n",
    "\n",
    "X = dat['tweet'].to_numpy()\n",
    "y = dat['label'].to_numpy()\n",
    "\n",
    "acc_values = []\n",
    "f1_scores = []\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "k = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"CROSS VALIDATION K = %d\" % (k))\n",
    "    k += 1\n",
    "    \n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    model = BertForSequenceClassification(num_labels)\n",
    "    model.to(device)\n",
    "    \n",
    "    optim1 = optim.Adam([\n",
    "            {\"params\":model.bert.parameters(), \"lr\": lrmain},\n",
    "            {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    optimizer_ft = optim1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataloaders_dict, dataset_sizes = get_dataloaders(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_kfold = train_model(model, criterion, optimizer_ft, num_epochs=epochs)\n",
    "    model_kfold.to(device)\n",
    "    model_kfold.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        Perform predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = X_test\n",
    "    labels = y_test\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True, max_length=64,\n",
    "                                             pad_to_max_length=True, return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler,\n",
    "                                       batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            \n",
    "        logits = outputs\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        \n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "    predictions = np.argmax(np.concatenate(predictions, axis=0), axis=1).flatten()\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    acc = accuracy_score(predictions, true_labels)\n",
    "    f1s = f1_score(predictions, true_labels)\n",
    "    \n",
    "    print(\"\\n>> TEST -  ACC: %.3f, F1-Score: %.3f\" % (acc, f1s))\n",
    "    \n",
    "    acc_values.append(acc)\n",
    "    f1_scores.append(f1s)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results = pd.DataFrame({\n",
    "    'acc': acc_values,\n",
    "    'f1_score': f1_scores\n",
    "})\n",
    "pd_results.to_csv('results/BERT.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0e2beced6842d8ec2d25a6dd6afaf46ee03c874f877553cf54f2fa9521603e8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
